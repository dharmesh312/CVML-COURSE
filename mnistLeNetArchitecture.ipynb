{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnistLeNetArchitecture.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/dharmesh312/CVML-COURSE/blob/master/mnistLeNetArchitecture.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "P9znQ7vaPBzX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rfe2jkU-QAAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "bd6ace79-57c5-455d-d024-93ef5a983194"
      },
      "cell_type": "code",
      "source": [
        "# get the data \n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\" ,one_hot = True, reshape = False)\n",
        "\n",
        "test_data_images  = mnist.test.images\n",
        "train_data_images = mnist.train.images\n",
        "valid_data_images = mnist.validation.images\n",
        "\n",
        "test_data_labels  = mnist.test.labels\n",
        "train_data_labels = mnist.train.labels\n",
        "valid_data_labels = mnist.validation.labels\n",
        "\n",
        "print (\"Train data shape :\" , train_data_images.shape)\n",
        "print (\"test data shape :\" , test_data_images.shape)\n",
        "print (\"Valid data shape :\" , valid_data_images.shape)\n",
        "print (\"Shape of a image is :\" , train_data_images[0].shape)\n",
        "\n",
        "# pre processing the data\n",
        "# since LeNet architectureis designed for 32 * 32 images and we have images that have dimensions of 28*28 so we need to pad the images \n",
        "test_data_images = np.pad(test_data_images , ((0,0) , (2,2) ,(2,2) , (0,0)) , 'constant' , constant_values=(0,0))\n",
        "train_data_images = np.pad(train_data_images , ((0,0) , (2,2) ,(2,2) , (0,0)) , 'constant' , constant_values=(0,0))\n",
        "valid_data_images = np.pad(valid_data_images , ((0,0) , (2,2) ,(2,2) , (0,0)) , 'constant' , constant_values=(0,0))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-bc8c76c991e8>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "Train data shape : (55000, 28, 28, 1)\n",
            "test data shape : (10000, 28, 28, 1)\n",
            "Valid data shape : (5000, 28, 28, 1)\n",
            "Shape of a image is : (28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QINIXW0oSHm9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# shuffle the training data to for normaliation\n",
        "from sklearn.utils import shuffle\n",
        "train_data_images , train_data_labels = shuffle(train_data_images,train_data_labels , random_state = 0) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ga-1xqvQSHlK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# generate convNet\n",
        "from tensorflow.contrib.layers import flatten\n",
        "\n",
        "def convNet(image):\n",
        "    \n",
        "    convLayer1 = {\"Weights\" : tf.Variable(tf.truncated_normal([5,5,1,6]  , mean = 0 ,stddev = 0.1))  , \"Biases\"  :  tf.Variable(tf.zeros(6)) }\n",
        "    convLayer2 = {\"Weights\" : tf.Variable(tf.truncated_normal([5,5,6,16] , mean = 0 ,stddev = 0.1))  , \"Biases\"  :  tf.Variable(tf.zeros(16)) }\n",
        "    fullyConnectedLayer1 = {\"Weights\" : tf.Variable(tf.truncated_normal( [400,120]  , mean = 0 ,stddev = 0.1 ))  , \"Biases\"  :  tf.Variable(tf.zeros(120)) }\n",
        "    fullyConnectedLayer2 = {\"Weights\" : tf.Variable(tf.truncated_normal( [120,84]  , mean = 0 ,stddev = 0.1))  , \"Biases\"  :  tf.Variable(tf.zeros(84)) }\n",
        "    outputLayer = {\"Weights\" : tf.Variable(tf.truncated_normal( [84,10] , mean = 0 ,stddev = 0.1 ))  , \"Biases\"  :  tf.Variable(tf.zeros(10)) }\n",
        "\n",
        "    #     layer1 computations  ip = 32*32 op =14*14*6 \n",
        "    conv1 = tf.nn.conv2d(image , convLayer1[\"Weights\"] , strides = [1,1,1,1] , padding = 'VALID') + convLayer1['Biases']\n",
        "    conv1 = tf.nn.relu(conv1)\n",
        "    pool1 = tf.nn.max_pool(conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
        "\n",
        "    #     layer2 computatations  ip = 14*14*6 op = 5*5*16\n",
        "    conv2 = tf.nn.conv2d(pool1 , convLayer2[\"Weights\"] , strides = [1,1,1,1] , padding = 'VALID') + convLayer2['Biases']\n",
        "    conv2 = tf.nn.relu(conv2)\n",
        "    pool2 = tf.nn.max_pool(conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
        "    \n",
        "    # layer3 computations . Fully connected layer 400 to 120\n",
        "    fc1 = flatten(pool2)\n",
        "    fc1 = tf.add( tf.matmul(fc1 , fullyConnectedLayer1[\"Weights\"]) , fullyConnectedLayer1[\"Biases\"] )\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    \n",
        "    # layer4 computations . Fully connected layer 120 to 84\n",
        "    fc2 = tf.add( tf.matmul(fc1 ,fullyConnectedLayer2[\"Weights\"]) , fullyConnectedLayer2[\"Biases\"] )\n",
        "    fc2 = tf.nn.relu(fc2)\n",
        "    \n",
        "    # layer5 computations . output layer\n",
        "    output = tf.add(tf.matmul(fc2 , outputLayer['Weights']) , outputLayer['Biases'])\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fPlKz0qmSHid",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# placeholders and other variables defination\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
        "y = tf.placeholder('float')\n",
        "rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x8G-WOKLSASj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_conv_net(x):\n",
        "  prediction = convNet(x)\n",
        "#   print (prediction.shape , oneHotY.shape)\n",
        "  cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( logits = prediction, labels = y) )\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate = rate).minimize(cost)\n",
        "  \n",
        "  correct = tf.equal(tf.argmax(prediction, 1) , tf.argmax(y,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct ,'float'))\n",
        "  \n",
        "  print (\"Starting training ....\" )\n",
        "  #taining the model\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(epochs):\n",
        "      train_data_image, train_data_label = shuffle(train_data_images, train_data_labels)\n",
        "      loss = 0\n",
        "\n",
        "      for start in range(0 , len(train_data_image) , batch_size):\n",
        "        last = start + batch_size\n",
        "        batch_x, batch_y = train_data_image[ start : last ], train_data_label[ start : last ]\n",
        "        c, l = sess.run([optimizer,cost] ,feed_dict = { x:batch_x , y:batch_y })\n",
        "        loss += l\n",
        "      print (\"Epoch:\" , i , \" with Loss :\" , loss) \n",
        "      print (\"Validation Set accuracy is :\", accuracy.eval({x : valid_data_images , y : valid_data_labels}))\n",
        "      \n",
        "    print (\"Test Set accuracy is :\", accuracy.eval({x : test_data_images , y : test_data_labels}))\n",
        "  \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N5nPyoL5bj26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "outputId": "c82074ad-8671-4961-ce2d-ba3986b5c751"
      },
      "cell_type": "code",
      "source": [
        "train_conv_net(x)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-20ee81cf4e03>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
            "\n",
            "Starting training ....\n",
            "Epoch: 0  with Loss : 155.29945572838187\n",
            "Validation Set accuracy is : 0.9654\n",
            "Epoch: 1  with Loss : 47.1043068934232\n",
            "Validation Set accuracy is : 0.9764\n",
            "Epoch: 2  with Loss : 32.507818980142474\n",
            "Validation Set accuracy is : 0.9828\n",
            "Epoch: 3  with Loss : 24.428047337802127\n",
            "Validation Set accuracy is : 0.984\n",
            "Epoch: 4  with Loss : 20.22345421463251\n",
            "Validation Set accuracy is : 0.985\n",
            "Epoch: 5  with Loss : 16.868207378080115\n",
            "Validation Set accuracy is : 0.9862\n",
            "Epoch: 6  with Loss : 13.655200758716092\n",
            "Validation Set accuracy is : 0.9832\n",
            "Epoch: 7  with Loss : 12.511252375668846\n",
            "Validation Set accuracy is : 0.9872\n",
            "Epoch: 8  with Loss : 10.583607610140461\n",
            "Validation Set accuracy is : 0.9896\n",
            "Epoch: 9  with Loss : 9.409778049506713\n",
            "Validation Set accuracy is : 0.9864\n",
            "Epoch: 10  with Loss : 7.807320025371155\n",
            "Validation Set accuracy is : 0.9882\n",
            "Epoch: 11  with Loss : 6.77922459342517\n",
            "Validation Set accuracy is : 0.989\n",
            "Epoch: 12  with Loss : 6.630497986901901\n",
            "Validation Set accuracy is : 0.9896\n",
            "Epoch: 13  with Loss : 5.357619385511498\n",
            "Validation Set accuracy is : 0.9892\n",
            "Epoch: 14  with Loss : 4.656608435092494\n",
            "Validation Set accuracy is : 0.9886\n",
            "Epoch: 15  with Loss : 5.275855362524453\n",
            "Validation Set accuracy is : 0.989\n",
            "Epoch: 16  with Loss : 4.5285329582548\n",
            "Validation Set accuracy is : 0.9898\n",
            "Epoch: 17  with Loss : 4.191743167739332\n",
            "Validation Set accuracy is : 0.9892\n",
            "Epoch: 18  with Loss : 3.2634286918128055\n",
            "Validation Set accuracy is : 0.9892\n",
            "Epoch: 19  with Loss : 4.239469659696624\n",
            "Validation Set accuracy is : 0.99\n",
            "Test Set accuracy is : 0.988\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}