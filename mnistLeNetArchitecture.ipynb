{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnistLeNetArchitecture.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/dharmesh312/CVML-COURSE/blob/master/mnistLeNetArchitecture.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "P9znQ7vaPBzX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rfe2jkU-QAAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "61dac626-16a2-4307-c25c-53bd45c35876"
      },
      "cell_type": "code",
      "source": [
        "# get the data \n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\" ,one_hot = True, reshape = False)\n",
        "\n",
        "test_data_images  = mnist.test.images\n",
        "train_data_images = mnist.train.images\n",
        "valid_data_images = mnist.validation.images\n",
        "\n",
        "test_data_labels  = mnist.test.labels\n",
        "train_data_labels = mnist.train.labels\n",
        "valid_data_labels = mnist.validation.labels\n",
        "\n",
        "print (\"Train data shape :\" , train_data_images.shape)\n",
        "print (\"test data shape :\" , test_data_images.shape)\n",
        "print (\"Valid data shape :\" , valid_data_images.shape)\n",
        "print (\"Shape of a image is :\" , train_data_images[0].shape)\n",
        "\n",
        "# pre processing the data\n",
        "# since LeNet architectureis designed for 32 * 32 images and we have images that have dimensions of 28*28 so we need to pad the images \n",
        "test_data_images = np.pad(test_data_images , ((0,0) , (2,2) ,(2,2) , (0,0)) , 'constant' , constant_values=(0,0))\n",
        "train_data_images = np.pad(train_data_images , ((0,0) , (2,2) ,(2,2) , (0,0)) , 'constant' , constant_values=(0,0))\n",
        "valid_data_images = np.pad(valid_data_images , ((0,0) , (2,2) ,(2,2) , (0,0)) , 'constant' , constant_values=(0,0))\n"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Train data shape : (55000, 28, 28, 1)\n",
            "test data shape : (10000, 28, 28, 1)\n",
            "Valid data shape : (5000, 28, 28, 1)\n",
            "Shape of a image is : (28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QINIXW0oSHm9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# shuffle the training data to for normaliation\n",
        "from sklearn.utils import shuffle\n",
        "train_data_images , train_data_labels = shuffle(train_data_images,train_data_labels , random_state = 0) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ga-1xqvQSHlK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# generate convNet\n",
        "from tensorflow.contrib.layers import flatten\n",
        "\n",
        "def convNet(image):\n",
        "    \n",
        "    convLayer1 = {\"Weights\" : tf.Variable(tf.truncated_normal([5,5,1,6]  , mean = 0 ,stddev = 0.1))  , \"Biases\"  :  tf.Variable(tf.zeros(6)) }\n",
        "    convLayer2 = {\"Weights\" : tf.Variable(tf.truncated_normal([5,5,6,16] , mean = 0 ,stddev = 0.1))  , \"Biases\"  :  tf.Variable(tf.zeros(16)) }\n",
        "    fullyConnectedLayer1 = {\"Weights\" : tf.Variable(tf.truncated_normal( [400,120]  , mean = 0 ,stddev = 0.1 ))  , \"Biases\"  :  tf.Variable(tf.zeros(120)) }\n",
        "    fullyConnectedLayer2 = {\"Weights\" : tf.Variable(tf.truncated_normal( [120,84]  , mean = 0 ,stddev = 0.1))  , \"Biases\"  :  tf.Variable(tf.zeros(84)) }\n",
        "    outputLayer = {\"Weights\" : tf.Variable(tf.truncated_normal( [84,10] , mean = 0 ,stddev = 0.1 ))  , \"Biases\"  :  tf.Variable(tf.zeros(10)) }\n",
        "\n",
        "    #     layer1 computations  ip = 32*32 op =14*14*6 \n",
        "    conv1 = tf.nn.conv2d(image , convLayer1[\"Weights\"] , strides = [1,1,1,1] , padding = 'VALID') + convLayer1['Biases']\n",
        "    conv1 = tf.nn.relu(conv1)\n",
        "    pool1 = tf.nn.max_pool(conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
        "\n",
        "    #     layer2 computatations  ip = 14*14*6 op = 5*5*16\n",
        "    conv2 = tf.nn.conv2d(pool1 , convLayer2[\"Weights\"] , strides = [1,1,1,1] , padding = 'VALID') + convLayer2['Biases']\n",
        "    conv2 = tf.nn.relu(conv2)\n",
        "    pool2 = tf.nn.max_pool(conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
        "    \n",
        "    # layer3 computations . Fully connected layer 400 to 120\n",
        "    fc1 = flatten(pool2)\n",
        "    fc1 = tf.add( tf.matmul(fc1 , fullyConnectedLayer1[\"Weights\"]) , fullyConnectedLayer1[\"Biases\"] )\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    \n",
        "    # layer4 computations . Fully connected layer 120 to 84\n",
        "    fc2 = tf.add( tf.matmul(fc1 ,fullyConnectedLayer2[\"Weights\"]) , fullyConnectedLayer2[\"Biases\"] )\n",
        "    fc2 = tf.nn.relu(fc2)\n",
        "    \n",
        "    # layer5 computations . output layer\n",
        "    output = tf.add(tf.matmul(fc2 , outputLayer['Weights']) , outputLayer['Biases'])\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fPlKz0qmSHid",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# placeholders and other variables defination\n",
        "epochs = 10\n",
        "batch_size = 50\n",
        "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
        "y = tf.placeholder('float')\n",
        "rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x8G-WOKLSASj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_conv_net(x):\n",
        "  prediction = convNet(x)\n",
        "#   print (prediction.shape , oneHotY.shape)\n",
        "  cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( logits = prediction, labels = y) )\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate = rate).minimize(cost)\n",
        "  \n",
        "  correct = tf.equal(tf.argmax(prediction, 1) , tf.argmax(y,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct ,'float'))\n",
        "  \n",
        "  print (\"Starting training ....\" )\n",
        "  #taining the model\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(epochs):\n",
        "      train_data_image, train_data_label = shuffle(train_data_images, train_data_labels)\n",
        "      loss = 0\n",
        "\n",
        "      for start in range(0 , len(train_data_image) , batch_size):\n",
        "        last = start + batch_size\n",
        "        batch_x, batch_y = train_data_image[ start : last ], train_data_label[ start : last ]\n",
        "        c, l = sess.run([optimizer,cost] ,feed_dict = { x:batch_x , y:batch_y })\n",
        "        loss += l\n",
        "      print (\"Epoch:\" , i , \" with Loss :\" , loss) \n",
        "      print (\"Validation Set accuracy is :\", accuracy.eval({x : valid_data_images , y : valid_data_labels}))\n",
        "      \n",
        "    print (\"Test Set accuracy is :\", accuracy.eval({x : test_data_images , y : test_data_labels}))\n",
        "  \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N5nPyoL5bj26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "02c78175-7a0c-4eb5-8d3f-b653390196fa"
      },
      "cell_type": "code",
      "source": [
        "train_conv_net(x)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training ....\n",
            "Epoch: 0  with Loss : 140.86496805772185\n",
            "Validation Set accuracy is : 0.9698\n",
            "Epoch: 1  with Loss : 43.43502079509199\n",
            "Validation Set accuracy is : 0.979\n",
            "Epoch: 2  with Loss : 31.117777061648667\n",
            "Validation Set accuracy is : 0.9836\n",
            "Epoch: 3  with Loss : 23.711920015048236\n",
            "Validation Set accuracy is : 0.9834\n",
            "Epoch: 4  with Loss : 19.14842969318852\n",
            "Validation Set accuracy is : 0.987\n",
            "Epoch: 5  with Loss : 16.031015777844004\n",
            "Validation Set accuracy is : 0.9856\n",
            "Epoch: 6  with Loss : 13.643697672290727\n",
            "Validation Set accuracy is : 0.9878\n",
            "Epoch: 7  with Loss : 11.935751435696147\n",
            "Validation Set accuracy is : 0.9874\n",
            "Epoch: 8  with Loss : 10.69697851059027\n",
            "Validation Set accuracy is : 0.9884\n",
            "Epoch: 9  with Loss : 9.522179822786711\n",
            "Validation Set accuracy is : 0.9874\n",
            "Test Set accuracy is : 0.9846\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}