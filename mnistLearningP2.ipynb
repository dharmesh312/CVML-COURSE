{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnistLearningP2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/dharmesh312/CVML-COURSE/blob/master/mnistLearningP2.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "p5_xJVyMdqH6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rTidbZ5sd-jM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "32435c15-34ce-4ed6-f39a-92032b81a14e"
      },
      "cell_type": "code",
      "source": [
        "# get the data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-a839aeb82f4b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wACGRDDDeDSd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# creating the network \n",
        "hnl1 = 500\n",
        "hnl2 = 500\n",
        "hnl3 = 500\n",
        "numberOfLabels = 10\n",
        "batch_size = 100\n",
        "\n",
        "x = tf.placeholder('float' , [None , 784])\n",
        "y = tf.placeholder('float' )\n",
        "\n",
        "def createNeuralNet(data):\n",
        "  hiddenLayer1 = {'Weights' : tf.Variable(tf.random_normal([784 ,hnl1])) , 'Biases' : tf.Variable(tf.random_normal([hnl1]))}\n",
        "  hiddenLayer2 = {'Weights' : tf.Variable(tf.random_normal([hnl1 ,hnl2])) , 'Biases' : tf.Variable(tf.random_normal([hnl2]))}\n",
        "  hiddenLayer3 = {'Weights' : tf.Variable(tf.random_normal([hnl2 ,hnl3])) , 'Biases' : tf.Variable(tf.random_normal([hnl3]))}\n",
        "  outputLayer = {'Weights' : tf.Variable(tf.random_normal([hnl3 , numberOfLabels ])) , 'Biases' : tf.Variable(tf.random_normal([numberOfLabels]))}\n",
        "  \n",
        "  layer1 = tf.add(tf.matmul(data , hiddenLayer1['Weights']) , hiddenLayer1['Biases'])\n",
        "  layer1 = tf.nn.relu(layer1) \n",
        "  \n",
        "  layer2 = tf.add(tf.matmul(layer1 , hiddenLayer2['Weights']) , hiddenLayer2['Biases'])\n",
        "  layer2 = tf.nn.relu(layer2)\n",
        "  \n",
        "  layer3 = tf.add(tf.matmul(layer2 , hiddenLayer3['Weights']) , hiddenLayer3['Biases'])\n",
        "  layer3 = tf.nn.relu(layer3)\n",
        "  \n",
        "  output = tf.add(tf.matmul(layer3 , outputLayer['Weights']) , outputLayer['Biases'])\n",
        "  \n",
        "  return output\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CUJuFRw-eDWV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# trainng on the neural net\n",
        "def train_neural_net(x):\n",
        "  prediction = createNeuralNet(x)\n",
        "  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = prediction , labels = y))\n",
        "  optimizier = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
        "  \n",
        "  epochsCount = 20\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "    \n",
        "    for i in range(epochsCount):\n",
        "      loss = 0\n",
        "      for j in range(int(mnist.train.num_examples / batch_size)):\n",
        "        train_x , train_y = mnist.train.next_batch(batch_size)\n",
        "        c,l=sess.run([optimizier , cost] , feed_dict = {x:train_x , y :train_y})  \n",
        "        loss += l\n",
        "      print (\"Epoch:\" , i , \" with Loss\" , loss)\n",
        "    correct = tf.equal(tf.argmax(prediction, 1) , tf.argmax(y,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct ,'float'))   \n",
        "    print (\"The accuracy on test set is\" , accuracy.eval({x :mnist.test.images , y : mnist.test.labels}) ) \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ux09HuyjuHUz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "d231c4b6-e012-4916-a0a1-0d812a624f56"
      },
      "cell_type": "code",
      "source": [
        "train_neural_net(x)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0  with Loss 1683728.6369018555\n",
            "Epoch: 1  with Loss 405772.3408050537\n",
            "Epoch: 2  with Loss 212912.95512485504\n",
            "Epoch: 3  with Loss 133048.61311775446\n",
            "Epoch: 4  with Loss 76451.14118552208\n",
            "Epoch: 5  with Loss 50015.50456875935\n",
            "Epoch: 6  with Loss 33438.37827159697\n",
            "Epoch: 7  with Loss 24393.784989967942\n",
            "Epoch: 8  with Loss 22154.840879731142\n",
            "Epoch: 9  with Loss 18506.056849205517\n",
            "Epoch: 10  with Loss 15635.952616825933\n",
            "Epoch: 11  with Loss 15083.801999320276\n",
            "Epoch: 12  with Loss 13696.900139167905\n",
            "Epoch: 13  with Loss 14862.454155520796\n",
            "Epoch: 14  with Loss 13316.331477614469\n",
            "Epoch: 15  with Loss 10601.969604803078\n",
            "Epoch: 16  with Loss 12227.528996676207\n",
            "Epoch: 17  with Loss 13286.478593051434\n",
            "Epoch: 18  with Loss 8465.545686296728\n",
            "Epoch: 19  with Loss 11157.224083542824\n",
            "The accuracy on test set is 0.9583\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}