{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "steganylysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dharmesh312/CVML-COURSE/blob/master/steganylysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "bOhi6xcLfNTT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import tensorflow\n",
        "import tensorflow as tf\n",
        "from numpy import *\n",
        "from PIL import Image\n",
        "from keras.callbacks import TensorBoard\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CUFQMW34gSKs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rwMKFEKggSbG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e2EtmH-KOrmv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install image_slicer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Ci5IdTcgScz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# slicing the images from 512*512 to 4 256*256 image\n",
        "import os, os.path\n",
        "import image_slicer\n",
        "import shutil\n",
        "\n",
        "op_path = 'drive/BOSSbase_1.01'\n",
        "\n",
        "imlist=os.listdir(op_path)  \n",
        "imlist.sort()\n",
        "im1=array(Image.open(op_path + '/' + imlist[0]))\n",
        "\n",
        "tiles = ()\n",
        "for i in range(len(imlist)):\n",
        "  tile = image_slicer.slice(op_path + '/' + imlist[i], 4, save=False)\n",
        "  image_slicer.save_tiles(tile, directory='drive/croppedImages/' ,prefix='cover.' + str(i))\n",
        "\n",
        "m,n=im1.shape[0:2]\n",
        "print (m,n)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9AUuDFWethgf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorboardcolab\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "44JtX6nxtmrc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorboardcolab import TensorBoardColab\n",
        "tbc = TensorBoardColab()\n",
        "summary_writer = tbc.get_writer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TkGmoWnRgSfc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, os.path\n",
        "trainDir = \"drive/Bbase/coverFinal/\"\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def normalize(nparray,alpha=-1):\n",
        "\n",
        "    # Normalization of our first convolutional layer\n",
        "\n",
        "    nparray = np.array(nparray)\n",
        "    sumation = nparray.sum()\n",
        "    nparray = nparray/sumation\n",
        "    nparray = np.array(nparray) # return a nparray\n",
        "    return nparray\n",
        "\n",
        "\n",
        "def label_image(img):\n",
        "  word = img.split(\".\")[0]\n",
        "  if word == \"stego\":\n",
        "    return [0,1]\n",
        "  elif word == \"cover\":\n",
        "    return [1,0]\n",
        "\n",
        "def create_train_data():\n",
        "    training_data = []\n",
        "\n",
        "    i  = 0\n",
        "    arr = os.listdir(trainDir)\n",
        "    random.shuffle(arr)\n",
        "    for img in tqdm(arr[:3000]):\n",
        "        label = label_image(img)\n",
        "        path = os.path.join(trainDir,img)\n",
        "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
        "        training_data.append( [normalize(np.array(img),-1) , np.array(label)])\n",
        "\n",
        "#     np.save('drive/train_data.npy', training_data)\n",
        "#     random.shuffle(training_data)\n",
        "    print (\"\\n The shape of the images is:\")\n",
        "    return training_data\n",
        "\n",
        "trainData = create_train_data()\n",
        "validationData = trainData[2600:2800]\n",
        "trainData1 = trainData[:2600]\n",
        "testDataEntire = trainData[2800:]\n",
        "trainDataImages = np.array([i[0] for i in trainData1]).reshape([-1,256,256,1])  \n",
        "trainDataLabels = np.array([i[1] for i in trainData1])\n",
        "validData = np.array([i[0] for i in validationData]).reshape([-1,256,256,1])  \n",
        "validLabels = np.array([i[1] for i in validationData])\n",
        "testDataImages = np.array([i[0] for i in testDataEntire]).reshape([-1,256,256,1])  \n",
        "testDataLabels = np.array([i[1] for i in testDataEntire])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hLZJSsjggSil",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# architecture\n",
        "\n",
        "from tensorflow.contrib.layers import flatten\n",
        "\n",
        "def residual_unit(layer , inputSize , outputSize ):\n",
        "      convLayerResNetBlock1 = {\"Weights\" : tf.Variable(tf.truncated_normal([3,3,inputSize, outputSize] , mean = 0 ,stddev = 0.1))  , \"Biases\"  :  tf.Variable(tf.zeros(inputSize)) }\n",
        "      convLayerResNetBlock2 = {\"Weights\" : tf.Variable(tf.truncated_normal([3,3,inputSize, outputSize] , mean = 0 ,stddev = 0.1))  , \"Biases\"  :  tf.Variable(tf.zeros(inputSize)) }\n",
        "      \n",
        "      stepL1 = tf.nn.conv2d(layer , convLayerResNetBlock1[\"Weights\"] , strides = [1,1,1,1] , padding = 'SAME') + convLayerResNetBlock1['Biases']\n",
        "      stepL1 = tf.nn.relu(stepL1)\n",
        "      stepL1 = tf.nn.max_pool(stepL1, ksize = [1,2,2,1], strides = [1,1,1,1], padding = 'SAME')\n",
        "      stepL1 = tf.layers.batch_normalization(stepL1)\n",
        "\n",
        "      stepL2 = tf.nn.conv2d(stepL1 , convLayerResNetBlock2[\"Weights\"] , strides = [1,1,1,1] , padding = 'SAME') + convLayerResNetBlock2['Biases']\n",
        "      stepL2 = tf.nn.relu(stepL2)\n",
        "      stepL2 = tf.nn.max_pool(stepL2, ksize = [1,2,2,1], strides = [1,1,1,1], padding = 'SAME')\n",
        "      stepL2 = tf.layers.batch_normalization(stepL2)\n",
        "      \n",
        "      return layer + stepL2\n",
        "      \n",
        "      \n",
        "      \n",
        "def convNet(image):\n",
        "      layerInResBlock = 1\n",
        "      convLayerRes = {\"Weights\" : tf.Variable(tf.truncated_normal([5,5,1,12]  , mean = 0 ,stddev = 0.1))}\n",
        "      convLayer1 = {\"Weights\" : tf.Variable(tf.truncated_normal([7,7,12,64] , mean = 0 ,stddev = 0.1))  , \"Biases\"  :  tf.Variable(tf.zeros(64)) } \n",
        "      dimensionIncreaseLayer1 = {\"Weights\" : tf.Variable(tf.truncated_normal([3,3,64,128] , mean = 0 ,stddev = 0.1))  , \"Biases\"  :  tf.Variable(tf.zeros(128)) } \n",
        "      dimensionIncreaseLayer2 = {\"Weights\" : tf.Variable(tf.truncated_normal([3,3,128,256] , mean = 0 ,stddev = 0.1))  , \"Biases\"  :  tf.Variable(tf.zeros(256)) } \n",
        "      dimensionIncreaseLayer3 = {\"Weights\" : tf.Variable(tf.truncated_normal([3,3,256,512] , mean = 0 ,stddev = 0.1))  , \"Biases\"  :  tf.Variable(tf.zeros(512)) } \n",
        "      outputLayer = {\"Weights\" : tf.Variable(tf.truncated_normal( [131072,2]  , mean = 0 ,stddev = 0.1 ))  , \"Biases\"  :  tf.Variable(tf.zeros(2)) }\n",
        "      \n",
        "#       ip 256*256*1 ,op = 256*256*12 // special prediction layer\n",
        "      convRes = tf.nn.conv2d(image , convLayerRes[\"Weights\"] , strides = [1,1,1,1] , padding = 'SAME') \n",
        "  \n",
        "#      ip = 256*256*12 op = 128*128*64 // first conv layer from paper\n",
        "      conv1 = tf.nn.conv2d(convRes , convLayer1[\"Weights\"] , strides = [1,1,1,1] , padding = 'SAME') + convLayer1['Biases']\n",
        "      activation1 = tf.nn.relu(conv1)\n",
        "      pool1 = tf.nn.max_pool(activation1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
        "      layer = tf.layers.batch_normalization(pool1)\n",
        "      \n",
        "#       ResNet Part Starts // n1 set of layers ip 128*128*64 = 64*64*128\n",
        "      for i in range(layerInResBlock):\n",
        "          layer = residual_unit(layer , 64 , 64)\n",
        "#     downsampling  \n",
        "      layer = tf.nn.conv2d(layer , dimensionIncreaseLayer1[\"Weights\"] , strides = [1,2,2,1] , padding = 'SAME') + dimensionIncreaseLayer1['Biases']\n",
        "      layer = tf.nn.relu(layer)\n",
        "      layer = tf.layers.batch_normalization(layer)\n",
        "\n",
        "#     n2 set of blocks 64*64*128 -> 32*32*256\n",
        "      for i in range(layerInResBlock):\n",
        "          layer = residual_unit(layer , 128 , 128)\n",
        "#     downsampling\n",
        "      layer = tf.nn.conv2d(layer , dimensionIncreaseLayer2[\"Weights\"] , strides = [1,2,2,1] , padding = 'SAME') + dimensionIncreaseLayer2['Biases']\n",
        "      layer = tf.nn.relu(layer)\n",
        "      layer = tf.layers.batch_normalization(layer)\n",
        "    \n",
        "    \n",
        "#     n3 set of blocks 32*32*256 -> 16*16*512\n",
        "      for i in range(layerInResBlock):\n",
        "          layer = residual_unit(layer , 256 , 256)\n",
        "#     downsampling\n",
        "      layer = tf.nn.conv2d(layer , dimensionIncreaseLayer3[\"Weights\"] , strides = [1,2,2,1] , padding = 'SAME') + dimensionIncreaseLayer3['Biases']\n",
        "      layer = tf.nn.relu(layer)\n",
        "      layer = tf.layers.batch_normalization(layer)      \n",
        "\n",
        "#     n4 set of blocks  16*16*512 - > 131072    \n",
        "      for i in range(layerInResBlock):\n",
        "          layer = residual_unit(layer , 512 , 512)\n",
        "      \n",
        "      layer = tf.nn.max_pool(layer, ksize = [1,2,2,1], strides = [1,1,1,1], padding = 'SAME')\n",
        "      fcc = flatten(layer)\n",
        "      \n",
        "      output = tf.add(tf.matmul(fcc , outputLayer['Weights']) , outputLayer['Biases'])\n",
        "\n",
        "\n",
        "      return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GSpM4KjkN9fl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch_size = 10\n",
        "x = tf.placeholder(tf.float32, (None, 256, 256, 1))\n",
        "y = tf.placeholder(tf.float32, shape=[None, 2])\n",
        "rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IWWE_ekXi3mG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print (tf.test.gpu_device_name())\n",
        "from tensorflow.python.client import device_lib\n",
        "print (device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_8QjsO8mOskR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle # mixing up or currently ordered data that might lead our network astray in training.\n",
        "\n",
        "def normalizeMykernel(nparray,alpha=-1):\n",
        "\n",
        "    # Normalization of our first convolutional layer\n",
        "\n",
        "    nparray = np.array(nparray)\n",
        "    nparray[2,2] = alpha # Evaluate the influence of this alpha\n",
        "    nparray = np.ma.array(nparray, mask=False)\n",
        "    nparray.mask[2,2] = True  #Mask the center so he will not appear in the normalisation\n",
        "    sumation = nparray.sum()\n",
        "    nparray = nparray/sumation\n",
        "    nparray = np.array(nparray) # return a nparray\n",
        "    return nparray\n",
        "\n",
        "\n",
        "def trainNetwork(x):\n",
        "#   print (prediction.shape , oneHotY.shape)\n",
        "  prediction = convNet(x)\n",
        "  cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( logits = prediction, labels = y) )\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)\n",
        "  \n",
        "  correct = tf.equal(tf.argmax(prediction, 1) , tf.argmax(y,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct ,'float'))\n",
        "  \n",
        "  print (\"Starting training ....\" )\n",
        "  #taining the model\n",
        "  with tf.Session() as sess:\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    for i in range(epochs):\n",
        "      train_data_image, train_data_label = shuffle(trainDataImages, trainDataLabels , random_state = 0)\n",
        "      loss = 0\n",
        "      \n",
        "#       initialiing the new values for the first vconv layer kernel according to the plan from the texas paper \n",
        "      gr = tf.get_default_graph()\n",
        "      tf.summary.FileWriter(gr)\n",
        "      conv1_kernel_val = gr.get_tensor_by_name('Variable:0').eval()      \n",
        "      normalizeMykernel(conv1_kernel_val,-1)\n",
        "      conv1_kernel_val = normalizeMykernel(conv1_kernel_val,-1)\n",
        "      sess.run(tf.assign( gr.get_tensor_by_name('Variable:0') , conv1_kernel_val))\n",
        "      \n",
        "      for start in range(0 , len(train_data_image) , batch_size):\n",
        "        last = start + batch_size\n",
        "        batch_x, batch_y = train_data_image[ start : last ], train_data_label[ start : last ]\n",
        "        c, l = sess.run([optimizer,cost] ,feed_dict = { x:batch_x , y:batch_y })\n",
        "        loss += l\n",
        "      \n",
        "      tbc.save_value(\"Loss per Epoch\", \"xyc\", i, loss)\n",
        "\n",
        "      print (\"Epoch:\" , i , \" with Loss :\" , loss) \n",
        "      \n",
        "      print (\"Validation Set accuracy is :\", accuracy.eval({x : validData , y : validLabels}))\n",
        "      tbc.save_value(\" validation accuracy\", \"asdasd\", i, accuracy.eval({x : validData , y : validLabels}))\n",
        "#       print (validLabels , sess.run(prediction,feed_dict = {x : validData}))\n",
        "      \n",
        "      \n",
        "    print (\"Test Set accuracy is :\", accuracy.eval({x : testDataImages , y : testDataLabels}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qTvf_4fzVBt1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainNetwork(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rbjkUZPNW0Sw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}